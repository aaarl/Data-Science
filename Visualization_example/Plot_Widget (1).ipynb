{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook\n",
    "\n",
    "Einfach alle Zellen ausführen, dann entsteht unten ein Widget indem ihr Plots von zwei Spalten gegeneinander anschauen könnt. \n",
    "Die Zelle unter \"Spark starten\" startet eine Spark Session, die Zelle unter \"Spark stoppen\" beendet diese. \n",
    "Ihr braucht die Spark Session für den Abschnitt \"Laden und in Pandas verwandeln\", das Widget an sich funktioniert ohne Spark. \n",
    "\n",
    "Es kann sein dass ihr nachdem ihr die Zelle unter \"Pandas installieren\" ausgeführt habt das Notebook neu starten müsst. (Details sind dort beschrieben). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark starten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "myAppName = \"plot-widget\" #Nur kleine Buchstaben hier, at any cost\n",
    "spark = pyspark.sql.SparkSession\\\n",
    "       .builder\\\n",
    "       .appName(myAppName)\\\n",
    "       .config(\"spark.kubernetes.executor.podNamePrefix\", os.environ[\"POD_NAME\"]+'-'+myAppName)\\\n",
    "       .config(\"spark.executor.cores\", \"2\")\\\n",
    "       .config(\"spark.executor.memory\", \"2g\")\\\n",
    "       .config(\"spark.driver.maxResultSize\", \"2g\")\\\n",
    "       .config(\"spark.dynamicAllocation.maxExecutors\", \"8\")\\\n",
    "       .config(\"spark.jars.packages\", \"\")\\\n",
    "       .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages installieren\n",
    "Um die Plots auszuführen wird das python package pandas und matplotlib benötigt. Es muss **einmal** für euren User installiert werden. Also beim ersten ausführen des Notebooks die folgende Zelle ausführen. Danach am besten auskommentieren ('#' davor) oder einfach löschen, ihr braucht sie nicht mehr. Es kann sein dass ihr danach das Notbeook neu starten müsst, links oben \"kernel --> Restart kernel\". Die Fehlermeldung \"ERROR: jupyterlab-git 0.21.0rc0 has requirement nbdime~=2.0, but you'll have nbdime 1.1.0 which is incompatible.\" is kein Problem so lange am Ende \"Successfully i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laden und in Pandas verwandeln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Der Maschinentyp ist RZM oder VE, nummer 300 - 303, einfach hinten in filepath ändern\n",
    "Machine = spark.read.parquet(\"/reif/01-data-corpus/303/RZM/filled.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Der Filter hier ist auf den 1.April 2020 gesetzt. Ihr könnt den anpassen, sollte klar sein wie. \n",
    "#Aber Achtung, wenn ihr den Zeitraum zu lang macht kanns sein dass die Konvertierung in Pandas nicht mehr funtioniert und der Plot wird unübersichtlich\n",
    "Machine_df = Machine.filter(F.col(\"timestamp\") >= \"2019-07-01 19:19:00\").filter(F.col(\"timestamp\") <= \"2019-07-02 15:59:59\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Die folgende Zelle is optional, sie wirft die Spalten raus die nur einen Wert schreiben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in Machine_df.columns:\n",
    "    if len(Machine_df[col].unique()) < 2:\n",
    "        Machine_df = Machine_df.drop(col, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot eines Tages\n",
    "Plottet die Werte aus zwei Spalten in den gleichen Plot. Wenn ihr \"Same Axes\" wählt wird für die Werte eine gemeinsame Skala (links) angelegt, bei \"Different Axes\" wird für den zweiten Plot rechts eine eigene Skala angelegt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbca17fe92c940afa004f596b04d3c5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='axis_types', options=('Different Axes', 'Same Axes'), value='Diffe…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact \n",
    "def hallowelt_1(axis_types = [\"Different Axes\", \"Same Axes\"], column1 = Machine_df.drop(\"timestamp\", axis = 1).columns, column2 = Machine_df.drop(\"timestamp\", axis = 1).columns): \n",
    "    fig, ax1 = plt.subplots(figsize=(20,10))\n",
    "    Machine_df.plot(x=\"timestamp\", y= column1, ax = ax1)\n",
    "    if axis_types == \"Same Axes\":\n",
    "        Machine_df.plot(x=\"timestamp\", y=column2, ax = ax1, color = \"orange\")\n",
    "    else:\n",
    "        ax1.legend(loc = \"upper left\")\n",
    "        ax2 = ax1.twinx()\n",
    "        Machine_df.plot(x=\"timestamp\", y=column2, ax = ax2, color = \"orange\")\n",
    "        ax2.legend(loc = \"upper right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "048ab09493494bea9376cefc3ba8f951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='column1', options=('Energie_Betriebsst_BSZ_1_Kommentar_Betriebsstu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact \n",
    "def hallowelt_2(column1 = Machine_df.drop(\"timestamp\", axis = 1).columns): \n",
    "    fig, ax1 = plt.subplots(figsize=(20,10))\n",
    "    Machine_df.plot(x=\"timestamp\", y= column1, ax = ax1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot des ganzen Jahres\n",
    "\n",
    "Die folgenden Zellen verwenden den oben geladenen \"Machine\" spark dataframe. Pro Tag werden 3 Werte geschrieben:\n",
    "- Der Maximalwert des Tages\n",
    "- Der Minimalwert des Tages\n",
    "- Der Durchschnittswert des Tages\n",
    "\n",
    "Es kann im interaktiven Teil dann eine Spalte ausgewählt werden und diese drei Werte werden über den Kompletten Datenzeitraum geplotet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exprs_max = {x: \"max\" for x in Machine.drop(\"timestamp\").columns}\n",
    "exprs_min = {x: \"min\" for x in Machine.drop(\"timestamp\").columns}\n",
    "exprs_avg = {x: \"avg\" for x in Machine.drop(\"timestamp\").columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Machine_max = Machine.withColumn(\"timestamp\", F.date_trunc(\"day\", \"timestamp\")).groupBy(\"timestamp\").agg(exprs_max).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Machine_min = Machine.withColumn(\"timestamp\", F.date_trunc(\"day\", \"timestamp\")).groupBy(\"timestamp\").agg(exprs_min).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Machine_avg = Machine.withColumn(\"timestamp\", F.date_trunc(\"day\", \"timestamp\")).groupBy(\"timestamp\").agg(exprs_avg).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact \n",
    "def hallowelt_1(column = Machine.drop(\"timestamp\").columns): \n",
    "    fig, ax1 = plt.subplots(figsize=(20,10))\n",
    "    Machine_max.plot(x=\"timestamp\", y= \"max(\" + column + \")\", ax = ax1)\n",
    "    Machine_min.plot(x=\"timestamp\", y= \"min(\" + column + \")\", ax = ax1)\n",
    "    Machine_avg.plot(x=\"timestamp\", y= \"avg(\" + column + \")\", ax = ax1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark stoppen\n",
    "\n",
    "Wenn Ihr das Notebook gerade nicht mehr braucht, bitte diese Zeile ausführen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
